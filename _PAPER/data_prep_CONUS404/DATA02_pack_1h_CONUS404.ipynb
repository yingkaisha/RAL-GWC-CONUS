{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611cd51-6603-4f8d-a6de-e9e18228125a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e93a0b-13cc-4985-bb48-f8be95f1cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import dask\n",
    "import zarr\n",
    "import numpy as np\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91c1094-aeb8-4d23-a93b-c980a1615191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca08fb-eeb9-4570-9a00-0baeacd41c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e668cef0-3b1a-415e-9fcf-f08e6fc1603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_datetimes(year: int) -> np.ndarray:\n",
    "    start = np.datetime64(f\"{year}-01-01T00:00:00\", \"ns\")\n",
    "    stop  = np.datetime64(f\"{year+1}-01-01T00:00:00\", \"ns\")  # exclusive\n",
    "    hours = np.arange(start, stop, np.timedelta64(1, \"h\"))\n",
    "    return hours  # dtype: datetime64[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b3eee-2f9d-4e6d-b7b9-33c8513a7b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87aa2822-61ed-4d05-adcf-06f1ef4080f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "dt_list = hourly_datetimes(year)\n",
    "flag_soil = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55adf44-fc52-4c4b-9bea-3d619f383b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf2aa8f-296c-4c47-ae5b-dcd8ec11ad07",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     file_collect\u001b[38;5;241m.\u001b[39mappend(ds)\n\u001b[1;32m     47\u001b[0m ds_year \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat(file_collect, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# merge all\u001b[39;00m\n\u001b[1;32m     51\u001b[0m ds_year \u001b[38;5;241m=\u001b[39m ds_year\u001b[38;5;241m.\u001b[39mdrop_vars([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRF_Q\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRF_Q_LC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRF_PWAT_LC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "base_dir = '/glade/campaign/ral/hap/ksha/DWC_data/CONUS_domain_GP/raw_404/'\n",
    "base_dir_extra = '/glade/campaign/ral/hap/ksha/DWC_data/CONUS_domain_GP/raw_404_new/'\n",
    "\n",
    "varname_4d = ['WRF_P', 'WRF_Q', 'WRF_T', 'WRF_U', 'WRF_V', 'WRF_Q_tot']\n",
    "\n",
    "if flag_soil:\n",
    "    ds_static = xr.open_zarr('/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/static/C404_GP_static_LAKE.zarr')\n",
    "    \n",
    "    land = ds_static[\"LANDMASK\"] # 1 = land, 0 = water\n",
    "    lake = ds_static['LAKEMASK'] # 1 = lake, 0 = land & ocean\n",
    "    ocean_mask = (land == 0) & (lake == 0) # 1 = ocean\n",
    "    \n",
    "    lat2d = ds_static[\"XLAT\"]\n",
    "    lon2d = ds_static[\"XLONG\"]\n",
    "    \n",
    "    # --- xESMF wants its grid as an xarray Dataset -------------------------------\n",
    "    src_grid = xr.Dataset(\n",
    "        {\n",
    "            \"lat\":  ((\"y\", \"x\"), lat2d.values),\n",
    "            \"lon\":  ((\"y\", \"x\"), lon2d.values),\n",
    "            \"mask\": ((\"y\", \"x\"), land.values), # 1 = keep, 0 = ignore\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Destination grid is the *same* geometry but **without** the mask\n",
    "    dst_grid = xr.Dataset({\"lat\": ((\"y\", \"x\"), lat2d.values),\n",
    "                           \"lon\": ((\"y\", \"x\"), lon2d.values)})\n",
    "    \n",
    "    regridder = xe.Regridder(\n",
    "        src_grid, dst_grid, \n",
    "        method = \"bilinear\",\n",
    "        extrap_method = \"nearest_s2d\",\n",
    "    )\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "fn_year = sorted(glob(base_dir+f'*{year}*.zarr')+glob(base_dir_extra+f'*{year}*.zarr')) #[:100]\n",
    "\n",
    "if len(fn_year) > 0:\n",
    "    file_collect = []\n",
    "    \n",
    "    for i_fn, fn in enumerate(fn_year):\n",
    "        ds = xr.open_zarr(fn)\n",
    "        ds['time'] = [dt_list[i_fn],]\n",
    "        file_collect.append(ds)\n",
    "        \n",
    "    ds_year = xr.concat(file_collect, dim='time')\n",
    "    raise\n",
    "    \n",
    "    # merge all\n",
    "    ds_year = ds_year.drop_vars(['WRF_Q', 'WRF_Q_LC', 'WRF_PWAT_LC'])\n",
    "    \n",
    "    ds_year['WRF_precip_025'] = ds_year['WRF_precip']**0.25\n",
    "    ds_year['WRF_radar_composite_025'] = ds_year['WRF_radar_composite']**0.25\n",
    "    ds_year['WRF_PWAT_05'] = ds_year['WRF_PWAT']**0.5\n",
    "    ds_year['WRF_Q_tot_05'] = ds_year['WRF_Q_tot']**0.5\n",
    "\n",
    "    if flag_soil:\n",
    "        # =================================================== #\n",
    "        # SMOIS handling\n",
    "        da_SMOIS = ds_year[\"WRF_SMOIS\"]\n",
    "        da_SMOIS_land = da_SMOIS.where(land == 1)\n",
    "        \n",
    "        da_SMOIS_filled = regridder(da_SMOIS_land, skipna=True,)\n",
    "        da_SMOIS_filled = da_SMOIS_filled.rename({'y': 'south_north', 'x': 'west_east'})\n",
    "        da_SMOIS_filled['south_north'] = da_SMOIS['south_north']\n",
    "        da_SMOIS_filled['west_east'] = da_SMOIS['west_east']\n",
    "    \n",
    "        # land vals in da_SMOIS_filled corrected by da_SMOIS\n",
    "        da_SMOIS_correct = xr.where(land == 1, da_SMOIS, da_SMOIS_filled)\n",
    "    \n",
    "        # ocean vals in da_SMOIS_filled corrected by 0.0\n",
    "        da_SMOIS_correct = xr.where(ocean_mask == 1, 0.0, da_SMOIS_correct)\n",
    "        da_SMOIS_correct = da_SMOIS_correct.transpose(\"time\", \"south_north\", \"west_east\")\n",
    "        \n",
    "        ds_year[\"WRF_SMOIS\"] = da_SMOIS_correct\n",
    "        \n",
    "        # =================================================== #\n",
    "        # TSLB handling\n",
    "        da_TSLB = ds_year[\"WRF_TSLB\"]\n",
    "        da_TSLB_land = da_TSLB.where(lake == 0)\n",
    "        \n",
    "        da_TSLB_filled = regridder(da_TSLB_land, skipna=True,)\n",
    "        da_TSLB_filled = da_TSLB_filled.rename({'y': 'south_north', 'x': 'west_east'})\n",
    "        da_TSLB_filled['south_north'] = da_TSLB['south_north']\n",
    "        da_TSLB_filled['west_east'] = da_TSLB['west_east']\n",
    "    \n",
    "        # non-lake vals in da_TSLB_filled corrected by da_TSLB\n",
    "        da_TSLB_correct = xr.where(lake == 0, da_TSLB, da_TSLB_filled)\n",
    "        da_TSLB_correct = da_TSLB_correct.transpose(\"time\", \"south_north\", \"west_east\")\n",
    "    \n",
    "        ds_year[\"WRF_TSLB\"] = da_TSLB_correct\n",
    "\n",
    "    # =================================================== #\n",
    "    # rechunk\n",
    "    ds_year = ds_year.chunk(\n",
    "        {\n",
    "            'time': 16, \n",
    "            'bottom_top': 12, \n",
    "            'pressure_approx': 12, \n",
    "            'south_north': 336, \n",
    "            'west_east': 336\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    varnames = list(ds_year.keys())\n",
    "    # zarr encodings\n",
    "    dict_encoding = {}\n",
    "    \n",
    "    chunk_size_3d = dict(chunks=(16, 336, 336))\n",
    "    chunk_size_4d = dict(chunks=(16, 12, 336, 336))\n",
    "    \n",
    "    compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "    \n",
    "    for i_var, var in enumerate(varnames):\n",
    "        if var in varname_4d:\n",
    "            dict_encoding[var] = {'compressor': compress, **chunk_size_4d}\n",
    "        else:\n",
    "            dict_encoding[var] = {'compressor': compress, **chunk_size_3d}\n",
    "    \n",
    "    save_name = f'/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_{year}.zarr'\n",
    "    # ds_year.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)\n",
    "    print(save_name)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "else:\n",
    "    print(f'Skip year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0943c-eddc-4fc4-af5d-98a8b88409f9",
   "metadata": {},
   "source": [
    "## Fill NaN for future vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0529da8-4034-4fe6-946a-b966b6d7baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "fn = f'/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_{year}.zarr'\n",
    "ds = xr.open_zarr(fn)\n",
    "\n",
    "# 1) Build the full hourly index for 2022\n",
    "full_time = pd.date_range(\n",
    "    f\"{year}-01-01 00:00\", f\"{year}-12-31 23:00\", freq=\"H\"\n",
    ")\n",
    "\n",
    "# 2) Reindex your dataset on that time axis (adds NaNs for missing times)\n",
    "ds_year = ds.reindex(time=full_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8b7297-1669-4ff4-bb21-9b25293a798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ds_year['time'], ds_year['WRF_GLW'].values[:, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fbcbad-fe65-4b40-b199-98316bff7c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c95d64-55d6-407b-b062-fd55f890b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2022_dummy.zarr\n"
     ]
    }
   ],
   "source": [
    "varname_4d = ['WRF_P', 'WRF_Q', 'WRF_T', 'WRF_U', 'WRF_V', 'WRF_Q_tot']\n",
    "# =================================================== #\n",
    "# rechunk\n",
    "ds_year = ds_year.chunk(\n",
    "    {\n",
    "        'time': 16, \n",
    "        'bottom_top': 12, \n",
    "        'pressure_approx': 12, \n",
    "        'south_north': 336, \n",
    "        'west_east': 336\n",
    "    }\n",
    ")\n",
    "\n",
    "varnames = list(ds_year.keys())\n",
    "# zarr encodings\n",
    "dict_encoding = {}\n",
    "\n",
    "chunk_size_3d = dict(chunks=(16, 336, 336))\n",
    "chunk_size_4d = dict(chunks=(16, 12, 336, 336))\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    if var in varname_4d:\n",
    "        dict_encoding[var] = {'compressor': compress, **chunk_size_4d}\n",
    "    else:\n",
    "        dict_encoding[var] = {'compressor': compress, **chunk_size_3d}\n",
    "\n",
    "save_name = f'/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_{year}_dummy.zarr'\n",
    "ds_year.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f746ba-e888-432a-a244-6dd4d95bd468",
   "metadata": {},
   "source": [
    "## Create dummy datasets for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d271ab-5ce9-4b5b-b5f3-9a7f4da0b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_year(ds: xr.Dataset, year: int) -> xr.Dataset:\n",
    "    # full hourly index for the year\n",
    "    full_time = pd.date_range(f\"{year}-01-01 00:00\", f\"{year}-12-31 23:00\", freq=\"H\")\n",
    "\n",
    "    # empty slice for that year (length 0 if ds has no data in that year)\n",
    "    ds_empty = ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "\n",
    "    # expand to full_time; new timestamps are filled with NaN\n",
    "    return ds_empty.reindex(time=full_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a7a2af-77a0-4a48-bd00-be4e4f2760d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... 2023 ... \n",
      "dummy data created\n",
      "/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2023_dummy.zarr\n",
      " ... 2024 ... \n",
      "dummy data created\n",
      "/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2024_dummy.zarr\n",
      " ... 2025 ... \n",
      "dummy data created\n",
      "/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2025_dummy.zarr\n"
     ]
    }
   ],
   "source": [
    "varname_4d = ['WRF_P', 'WRF_Q', 'WRF_T', 'WRF_U', 'WRF_V', 'WRF_Q_tot']\n",
    "# =================================================== #\n",
    "\n",
    "year = 2020\n",
    "fn = f'/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_{year}.zarr'\n",
    "ds = xr.open_zarr(fn)\n",
    "\n",
    "for year in range(2023, 2026):\n",
    "    \n",
    "    print(f' ... {year} ... ')\n",
    "    \n",
    "    ds_year = make_dummy_year(ds, year)\n",
    "\n",
    "    print('dummy data created')\n",
    "    \n",
    "    ds_year = ds_year.chunk(\n",
    "        {\n",
    "            'time': 16, \n",
    "            'bottom_top': 12, \n",
    "            'pressure_approx': 12, \n",
    "            'south_north': 336, \n",
    "            'west_east': 336\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    varnames = list(ds_year.keys())\n",
    "    # zarr encodings\n",
    "    dict_encoding = {}\n",
    "    \n",
    "    chunk_size_3d = dict(chunks=(16, 336, 336))\n",
    "    chunk_size_4d = dict(chunks=(16, 12, 336, 336))\n",
    "    \n",
    "    compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "    \n",
    "    for i_var, var in enumerate(varnames):\n",
    "        if var in varname_4d:\n",
    "            dict_encoding[var] = {'compressor': compress, **chunk_size_4d}\n",
    "        else:\n",
    "            dict_encoding[var] = {'compressor': compress, **chunk_size_3d}\n",
    "    \n",
    "    save_name = f'/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_{year}_dummy.zarr'\n",
    "    ds_year.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)\n",
    "    print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da895fc-4adb-4231-bab1-dfb84b96cc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5bf0d2-15fd-49bf-9410-16e0f46f3d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "073bcc69-a579-48d1-8886-56381f2d07aa",
   "metadata": {},
   "source": [
    "## QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35bd7a5-cbfa-46e4-a336-9c80225a1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old = xr.open_zarr('/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2022.zarr')\n",
    "ds_new = xr.open_zarr('/glade/derecho/scratch/ksha/DWC_data/CONUS_domain_GP/C404/C404_GP_2022_dummy.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a480ee80-238e-4547-8070-be49f6993b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_old = ds_old['WRF_GLW'].values[:, 100, 100]\n",
    "val_new = ds_new['WRF_GLW'].values[:, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12042262-4952-41b2-b9af-2399338d4b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
